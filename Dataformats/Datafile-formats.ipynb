{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File formats\n",
    "\n",
    "### numpy, matlab etc binary: **.npy, .npz or .mat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['text'] = 'Something'\n",
    "data['array'] = np.zeros((10,10))\n",
    "\n",
    "np.savez('data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Data Format (**HDF5**) - https://www.hdfgroup.org/solutions/hdf5/\n",
    "\n",
    "Because it uses B-trees to index table objects, HDF5 works well for time series data such as stock price series, network monitoring data, and 3D meteorological data. The bulk of the data goes into straightforward arrays (the table objects) that can be accessed much more quickly than the rows of an SQL database, but B-tree access is available for non-array data. The HDF5 data storage mechanism can be simpler and faster than an SQL star schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simulations and numerical calculations (or gathered data) -> lot of data\n",
    "* Scanning through it takes up time\n",
    "* HDD blocks and sectors\n",
    "* Storing large data efficiently\n",
    "* B-tree search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good reads: \n",
    "\n",
    "* Numpy npz versus hdf5: https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr\n",
    "* Why not to use hdf5: https://cyrille.rossant.net/moving-away-hdf5/\n",
    "\n",
    "About B-trees:\n",
    "https://www.youtube.com/watch?v=aZjYr87r1b8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Disadvantages:\n",
    "  * Can't use grep, awk ... \n",
    "  * deleting slices won't make its size smaller\n",
    "  * corrupt data corrupts the whole file\n",
    "\n",
    "* [trillion particle plasma physics simulation](https://www.hdfgroup.org/trillion-particle/), which needed to store HDF5 files of 40 terabytes or more, and had to be able to sustainably write data at rates exceeding 50 gigabytes per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "data = np.random.random((100, 100, 100))\n",
    "\n",
    "with h5py.File('test.hdf', 'w') as outfile:\n",
    "    dset = outfile.create_dataset('a_descriptive_name', data=data, chunks=True)\n",
    "    dset.attrs['some key'] = 'Did you want some metadata?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network Common Data Form (**netcdf**) - https://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
